# ğŸ“˜ Agentic Document Question Answering System

## ğŸ” Overview
This project implements an **Agentic Document Question Answering (QA) system** that enables users to query documents and receive accurate, contextâ€‘grounded answers.

The system follows a modern **Retrievalâ€‘Augmented Generation (RAG)** architecture and is built using **agentâ€‘based reasoning with LangGraph**, a **vector database for semantic search**, and **local LLM inference using Ollama**.

This project was developed as part of an **Internship Technical Task** and prioritizes:

- Local inference  
- Clean architecture  
- Containerized deployment  
- Reproducible evaluation  

---

## ğŸš€ Key Features
- ğŸ”— Agentic workflow using **LangGraph**  
- ğŸ“„ PDF document ingestion & chunking  
- ğŸ§  Semantic search with vector embeddings  
- ğŸ—ƒï¸ **Weaviate** vector database  
- ğŸ¤– Local LLM inference via **Ollama**  
- ğŸŒ **FastAPI**â€‘based REST API  
- ğŸ³ **Docker & Docker Compose** support  
- ğŸ“‘ Swagger / OpenAPI documentation  

---

## ğŸ§° Tech Stack

| Component        | Technology        |
|------------------|------------------|
| Agent Framework  | LangGraph        |
| LLM Framework    | LangChain        |
| Vector Store     | Weaviate         |
| LLM Inference    | Ollama (Local)   |
| API Server       | FastAPI          |
| Containerization | Docker, Docker Compose |

---

## ğŸ§  System Architecture

### Highâ€‘Level Flow
User â”‚ â–¼ FastAPI (/query) â”‚ â–¼ LangGraph Agent â”‚ â”œâ”€â”€â–º Retriever â”€â”€â–º Weaviate (Vector Store) â”‚ â””â”€â”€â–º Reasoning â”€â”€â–º Ollama (Local LLM) â”‚ â–¼ Final Answer

### Architecture Description
1. Documents are ingested and split into chunks  
2. Embeddings are generated and stored in Weaviate  
3. User submits a query through FastAPI  
4. LangGraph agent retrieves relevant document chunks  
5. Local LLM generates a grounded response using retrieved context  

---

## ğŸ“ Project Structure
 â”œâ”€â”€ app/ â”‚   â”œâ”€â”€ agent.py        # LangGraph agent logic â”‚   â”œâ”€â”€ retriever.py    # Vector search logic â”‚   â”œâ”€â”€ vectorstore.py  # Weaviate connection â”‚   â”œâ”€â”€ ingest.py       # Document ingestion pipeline â”‚   â”œâ”€â”€ llm.py          # Ollama LLM configuration â”‚   â””â”€â”€ main.py         # FastAPI application â”œâ”€â”€ data/ â”‚   â””â”€â”€ documents/      # Input PDFs â”œâ”€â”€ run_ingest.py       # Manual ingestion script â”œâ”€â”€ Dockerfile â”œâ”€â”€ docker-compose.yml â”œâ”€â”€ requirements.txt â””â”€â”€ README.md

---

## âš™ï¸ Setup & Execution

### Prerequisites
- Docker & Docker Compose  
- Ollama installed locally  

---

## ğŸ§  Local LLM (Ollama)

The project uses **local inference** via Ollama.

### Start Ollama on Host Machine
```bash
ollama run llama3.2:1b

Ollama is not containerized intentionally.
Docker containers communicate with Ollama via:
http://host.docker.internal:11434

ğŸ³ Docker Deployment
Build & Start Services
docker compose up --build

(Optional) Ingest Documents
docker compose exec api python run_ingest.py

ğŸ”— Access Services
| Service  | URL  | 
|API Docs  | http://localhost:8000/docs | 
| Weaviate | http://localhost:8080 | 

ğŸ”Œ API Usage
Endpoint
POST /query



Query Parameter
- question â€“ string (required)
Example Request
curl -X POST \
  "http://localhost:8000/query?question=What is the document about?"


Example Response
{
  "question": "What is the document about?",
  "answer": "The document describes guidelines and operational rules related to..."
}

ğŸ“Š Evaluation 

This project follows a Retrieval-Augmented Generation (RAG) evaluation methodology using the **RAGAS** framework.

### Metrics Used
The following metrics are defined for evaluation:

- **Context Precision** â€“ Measures how much of the retrieved context is relevant
- **Context Recall** â€“ Measures whether relevant context was retrieved
- **Faithfulness** â€“ Ensures answers are grounded in retrieved documents
- **Answer Relevancy** â€“ Measures how well the answer addresses the user query

### Evaluation Script
An evaluation script is provided at:

```bash
evaluation/evaluate_rag.py

### HyDE Retrieval 

The retrieval pipeline was enhanced using **HyDE (Hypothetical Document Embeddings)**.
Instead of directly embedding the user query, the system first generates a hypothetical answer using the local LLM and performs retrieval using that generated text.

This improves semantic recall and contextual relevance, especially for abstract questions.

In the folder their is hyde_retriever.py file which is the Implementation of the Hypothetical Document Embeddings and in the agent.py file i have commented migration to the hyde pipeline code currently it is basic document ingestion and retreival , if need to change to the hyDE pipeline just uncomment these lines and comment the need lines of codes and re-run , then it good to go and will be successfully functioning with the Hypothetical Document Embeddings System.







